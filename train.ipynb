{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-28T15:07:21.831768Z",
     "start_time": "2024-04-28T15:07:19.437461Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import torch.utils.data as torchdata\n",
    "from torchaudio.transforms import AmplitudeToDB, MelSpectrogram\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import librosa\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from utils.metrics import calculate_metrics, metrics_to_string, calculate_competition_metrics\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from timm.scheduler import CosineLRScheduler\n",
    "from utils.init_utils import AverageMeter\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Init Utils",
   "id": "80c0d13f64ed7084"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T15:07:23.392257Z",
     "start_time": "2024-04-28T15:07:23.387752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def init_logger(log_file='train.log'):\n",
    "    from logging import INFO, FileHandler, Formatter, StreamHandler, getLogger\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ],
   "id": "10c282545bc83cdc",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CNN",
   "id": "a085f5997cd65c4d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T15:07:23.857623Z",
     "start_time": "2024-04-28T15:07:23.851293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class NormalizeMelSpec(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, X):\n",
    "        mean = X.mean((1, 2), keepdim=True)\n",
    "        std = X.std((1, 2), keepdim=True)\n",
    "        Xstd = (X - mean) / (std + self.eps)\n",
    "        norm_min, norm_max = \\\n",
    "            Xstd.min(-1)[0].min(-1)[0], Xstd.max(-1)[0].max(-1)[0]\n",
    "        fix_ind = (norm_max - norm_min) > self.eps * torch.ones_like(\n",
    "            (norm_max - norm_min)\n",
    "        )\n",
    "        V = torch.zeros_like(Xstd)\n",
    "        if fix_ind.sum():\n",
    "            V_fix = Xstd[fix_ind]\n",
    "            norm_max_fix = norm_max[fix_ind, None, None]\n",
    "            norm_min_fix = norm_min[fix_ind, None, None]\n",
    "            V_fix = torch.max(\n",
    "                torch.min(V_fix, norm_max_fix),\n",
    "                norm_min_fix,\n",
    "            )\n",
    "            V_fix = (V_fix - norm_min_fix) / (norm_max_fix - norm_min_fix)\n",
    "            V[fix_ind] = V_fix\n",
    "        return V\n",
    "\n",
    "\n",
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6):\n",
    "        super(GeM, self).__init__()\n",
    "        self.p = torch.nn.Parameter(torch.ones(1) * p)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.avg_pool2d(x.clamp(min=self.eps).pow(self.p), (x.size(-2), x.size(-1))).pow(1.0 / self.p)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        mel_spec_params = config[\"mel_spec_params\"]\n",
    "        self.logmelspec_extractor = nn.Sequential(\n",
    "                MelSpectrogram(\n",
    "                    sample_rate=mel_spec_params[\"sample_rate\"],\n",
    "                    n_mels=mel_spec_params[\"n_mels\"],\n",
    "                    f_min=mel_spec_params[\"f_min\"],\n",
    "                    f_max=mel_spec_params[\"f_max\"],\n",
    "                    n_fft=mel_spec_params[\"n_fft\"],\n",
    "                    hop_length=mel_spec_params[\"hop_length\"],\n",
    "                    normalized=True,\n",
    "                ),\n",
    "                AmplitudeToDB(top_db=80.0),\n",
    "                NormalizeMelSpec(),\n",
    "            )\n",
    "        \n",
    "        \n",
    "        out_indices = (3, 4)\n",
    "        self.backbone = timm.create_model(\n",
    "            config[\"backbone\"],\n",
    "            features_only=True,\n",
    "            pretrained=config[\"pretrained\"],\n",
    "            in_chans=config[\"in_chans\"],\n",
    "            num_classes=0,\n",
    "            out_indices=out_indices,\n",
    "        )\n",
    "        feature_dims = self.backbone.feature_info.channels()\n",
    "        print(f\"feature dims: {feature_dims}\")\n",
    "        self.global_pools = torch.nn.ModuleList([GeM() for _ in out_indices])\n",
    "        self.mid_features = np.sum(feature_dims)\n",
    "        self.neck = torch.nn.BatchNorm1d(self.mid_features)\n",
    "        self.head = nn.Linear(self.mid_features, len(config[\"target_columns\"]))\n",
    "        \n",
    "    def backbone_pass(self, x):\n",
    "        \n",
    "        x = self.logmelspec_extractor(x[\"wave\"]).unsqueeze(1)\n",
    "        ms = self.backbone(x)\n",
    "        h = torch.cat([global_pool(m) for m, global_pool in zip(ms, self.global_pools)], dim=1)\n",
    "        features = self.neck(h)\n",
    "        features = self.head(features)\n",
    "        x[\"logit\"] = features\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "            "
   ],
   "id": "8a08e61e1b54df01",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset",
   "id": "e41d458fb9a06547"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T15:07:24.230413Z",
     "start_time": "2024-04-28T15:07:24.224942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BirdDataset(torchdata.Dataset):\n",
    "\n",
    "    def __init__(self, df, config, num_classes, add_secondary_labels = False):\n",
    "        self.df = df\n",
    "        self.bird2id = config['bird2id']\n",
    "        self.period = config['period']\n",
    "        self.secondary_coef = config['secondary_coef']\n",
    "        self.df[\"secondary_labels\"] = (\n",
    "                self.df[\"secondary_labels\"]\n",
    "                .map(\n",
    "                    lambda s: s.replace(\"[\", \"\")\n",
    "                    .replace(\"]\", \"\")\n",
    "                    .replace(\",\", \"\")\n",
    "                    .replace(\"'\", \"\")\n",
    "                    .split(\" \")\n",
    "                ).values\n",
    "            )\n",
    "        \n",
    "        self.smooth_label = config['smooth_label']\n",
    "        self.num_classes = num_classes\n",
    "        self.add_secondary_labels = add_secondary_labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    \n",
    "    def prepare_target(self, idx):\n",
    "        target = np.zeros(self.num_classes, dtype=np.float32)\n",
    "        if self.df[\"primary_label\"].iloc[idx] != 'nocall':\n",
    "            primary_label = self.bird2id[self.df[\"primary_label\"].iloc[idx]]\n",
    "            target[primary_label] = 1.0\n",
    "            if self.add_secondary_labels:\n",
    "                for s in self.df[\"secondary_labels\"].iloc[idx]:\n",
    "                    if s != \"\" and s in self.bird2id.keys():\n",
    "                        target[self.bird2id[s]] = self.secondary_coef\n",
    "        target = torch.from_numpy(target).float()\n",
    "        return target\n",
    "\n",
    "    \n",
    "    def load_wave_and_crop(self, filename, period, start=None):\n",
    "\n",
    "        waveform_orig, sample_rate = librosa.load(filename, sr=32000, mono=False)\n",
    "    \n",
    "        wave_len = len(waveform_orig)\n",
    "        waveform = np.concatenate([waveform_orig, waveform_orig, waveform_orig])\n",
    "    \n",
    "        effective_length = sample_rate * period\n",
    "        while len(waveform) < (period * sample_rate * 3):\n",
    "            waveform = np.concatenate([waveform, waveform_orig])\n",
    "        if start is not None:\n",
    "            start = start - (period - 5) / 2 * sample_rate\n",
    "            while start < 0:\n",
    "                start += wave_len\n",
    "            start = int(start)\n",
    "        else:\n",
    "            if wave_len < effective_length:\n",
    "                start = np.random.randint(effective_length - wave_len)\n",
    "            elif wave_len > effective_length:\n",
    "                start = np.random.randint(wave_len - effective_length)\n",
    "            elif wave_len == effective_length:\n",
    "                start = 0\n",
    "    \n",
    "        waveform_seg = waveform[start: start + int(effective_length)]\n",
    "    \n",
    "        return waveform_orig, waveform_seg, sample_rate, start\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.df[\"path\"].iloc[idx]\n",
    "        \n",
    "        waveform_orig, waveform_seg, sample_rate, start = self.load_wave_and_crop(path, period=5, start=0)\n",
    "        waveform_seg = torch.from_numpy(np.nan_to_num(waveform_seg)).float()\n",
    "        rating = self.df[\"rating\"].iloc[idx]\n",
    "        target = self.prepare_target(idx)\n",
    "        \n",
    "        batch_dict = {\n",
    "            \"wave\": waveform_seg,\n",
    "            \"rating\": rating,\n",
    "            \"primary_targets\": (target > 0.5).float(),\n",
    "            \"smooth_targets\": target * (1-self.smooth_label) + self.smooth_label / target.size(-1),\n",
    "        }\n",
    "        \n",
    "        return batch_dict\n"
   ],
   "id": "466db0b5b58d5669",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Config",
   "id": "c76b9a869ded4d05"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T15:07:24.659550Z",
     "start_time": "2024-04-28T15:07:24.648684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config = {\n",
    "    \"mel_spec_params\": {\n",
    "                \"sample_rate\": 32000,\n",
    "                \"n_mels\": 128,\n",
    "                \"f_min\": 20,\n",
    "                \"f_max\": 16000,\n",
    "                \"n_fft\": 2048,\n",
    "                \"hop_length\": 512,\n",
    "                \"normalized\": True,\n",
    "                \"top_db\": 80,\n",
    "            },\n",
    "    \"seed\" : 42,\n",
    "    \"secondary_coef\" : 1.0,\n",
    "    \"smooth_label\" : 0.05,\n",
    "    \"period\" : 5,\n",
    "    \"backbone\": \"eca_nfnet_l0\",\n",
    "    \"pretrained\": True,\n",
    "    \"fold\": 3,\n",
    "    \"in_chans\": 1,\n",
    "    \n",
    "    \"output_folder\" : \"outputs\",\n",
    "    \"exp_name\": \"EXP3\",\n",
    "    \n",
    "    \"device\": get_device(),\n",
    "    \"apex\" : True, \n",
    "    \"max_grad_norm\" : 10,\n",
    "    \n",
    "    \"early_stopping\" : 10,\n",
    "    \"epochs\" : 120,\n",
    "    \n",
    "    \"train_loader_config\": {\n",
    "        \"batch_size\": 32,\n",
    "        \"num_workers\": 8,\n",
    "        \"pin_memory\": True,\n",
    "        \"drop_last\": True,\n",
    "    },\n",
    "    \"val_loader_config\": {\n",
    "        \"batch_size\": 64,\n",
    "        \"num_workers\": 8,\n",
    "        \"pin_memory\": True,\n",
    "        \"drop_last\": False,\n",
    "    },\n",
    "    \n",
    "    \"lr_max\" : 2.5e-4,\n",
    "    \"lr_min\" : 1e-7,\n",
    "    \"weight_decay\" : 1e-6,\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"target_columns\" : ['asbfly', 'ashdro1', 'ashpri1', 'ashwoo2', 'asikoe2', 'asiope1', 'aspfly1', 'aspswi1', 'barfly1', 'barswa', 'bcnher', 'bkcbul1', 'bkrfla1', 'bkskit1', 'bkwsti', 'bladro1', 'blaeag1', 'blakit1', 'blhori1', 'blnmon1', 'blrwar1', 'bncwoo3', 'brakit1', 'brasta1', 'brcful1', 'brfowl1', 'brnhao1', 'brnshr', 'brodro1', 'brwjac1', 'brwowl1', 'btbeat1', 'bwfshr1', 'categr', 'chbeat1', 'cohcuc1', 'comfla1', 'comgre', 'comior1', 'comkin1', 'commoo3', 'commyn', 'compea', 'comros', 'comsan', 'comtai1', 'copbar1', 'crbsun2', 'cregos1', 'crfbar1', 'crseag1', 'dafbab1', 'darter2', 'eaywag1', 'emedov2', 'eucdov', 'eurbla2', 'eurcoo', 'forwag1', 'gargan', 'gloibi', 'goflea1', 'graher1', 'grbeat1', 'grecou1', 'greegr', 'grefla1', 'grehor1', 'grejun2', 'grenig1', 'grewar3', 'grnsan', 'grnwar1', 'grtdro1', 'gryfra', 'grynig2', 'grywag', 'gybpri1', 'gyhcaf1', 'heswoo1', 'hoopoe', 'houcro1', 'houspa', 'inbrob1', 'indpit1', 'indrob1', 'indrol2', 'indtit1', 'ingori1', 'inpher1', 'insbab1', 'insowl1', 'integr', 'isbduc1', 'jerbus2', 'junbab2', 'junmyn1', 'junowl1', 'kenplo1', 'kerlau2', 'labcro1', 'laudov1', 'lblwar1', 'lesyel1', 'lewduc1', 'lirplo', 'litegr', 'litgre1', 'litspi1', 'litswi1', 'lobsun2', 'maghor2', 'malpar1', 'maltro1', 'malwoo1', 'marsan', 'mawthr1', 'moipig1', 'nilfly2', 'niwpig1', 'nutman', 'orihob2', 'oripip1', 'pabflo1', 'paisto1', 'piebus1', 'piekin1', 'placuc3', 'plaflo1', 'plapri1', 'plhpar1', 'pomgrp2', 'purher1', 'pursun3', 'pursun4', 'purswa3', 'putbab1', 'redspu1', 'rerswa1', 'revbul', 'rewbul', 'rewlap1', 'rocpig', 'rorpar', 'rossta2', 'rufbab3', 'ruftre2', 'rufwoo2', 'rutfly6', 'sbeowl1', 'scamin3', 'shikra1', 'smamin1', 'sohmyn1', 'spepic1', 'spodov', 'spoowl1', 'sqtbul1', 'stbkin1', 'sttwoo1', 'thbwar1', 'tibfly3', 'tilwar1', 'vefnut1', 'vehpar1', 'wbbfly1', 'wemhar1', 'whbbul2', 'whbsho3', 'whbtre1', 'whbwag1', 'whbwat1', 'whbwoo2', 'whcbar1', 'whiter2', 'whrmun', 'whtkin2', 'woosan', 'wynlau1', 'yebbab1', 'yebbul3', 'zitcis1'],\n",
    "    \n",
    "    \"bird2id\" : {'asbfly': 0, 'ashdro1': 1, 'ashpri1': 2, 'ashwoo2': 3, 'asikoe2': 4, 'asiope1': 5, 'aspfly1': 6, 'aspswi1': 7, 'barfly1': 8, 'barswa': 9, 'bcnher': 10, 'bkcbul1': 11, 'bkrfla1': 12, 'bkskit1': 13, 'bkwsti': 14, 'bladro1': 15, 'blaeag1': 16, 'blakit1': 17, 'blhori1': 18, 'blnmon1': 19, 'blrwar1': 20, 'bncwoo3': 21, 'brakit1': 22, 'brasta1': 23, 'brcful1': 24, 'brfowl1': 25, 'brnhao1': 26, 'brnshr': 27, 'brodro1': 28, 'brwjac1': 29, 'brwowl1': 30, 'btbeat1': 31, 'bwfshr1': 32, 'categr': 33, 'chbeat1': 34, 'cohcuc1': 35, 'comfla1': 36, 'comgre': 37, 'comior1': 38, 'comkin1': 39, 'commoo3': 40, 'commyn': 41, 'compea': 42, 'comros': 43, 'comsan': 44, 'comtai1': 45, 'copbar1': 46, 'crbsun2': 47, 'cregos1': 48, 'crfbar1': 49, 'crseag1': 50, 'dafbab1': 51, 'darter2': 52, 'eaywag1': 53, 'emedov2': 54, 'eucdov': 55, 'eurbla2': 56, 'eurcoo': 57, 'forwag1': 58, 'gargan': 59, 'gloibi': 60, 'goflea1': 61, 'graher1': 62, 'grbeat1': 63, 'grecou1': 64, 'greegr': 65, 'grefla1': 66, 'grehor1': 67, 'grejun2': 68, 'grenig1': 69, 'grewar3': 70, 'grnsan': 71, 'grnwar1': 72, 'grtdro1': 73, 'gryfra': 74, 'grynig2': 75, 'grywag': 76, 'gybpri1': 77, 'gyhcaf1': 78, 'heswoo1': 79, 'hoopoe': 80, 'houcro1': 81, 'houspa': 82, 'inbrob1': 83, 'indpit1': 84, 'indrob1': 85, 'indrol2': 86, 'indtit1': 87, 'ingori1': 88, 'inpher1': 89, 'insbab1': 90, 'insowl1': 91, 'integr': 92, 'isbduc1': 93, 'jerbus2': 94, 'junbab2': 95, 'junmyn1': 96, 'junowl1': 97, 'kenplo1': 98, 'kerlau2': 99, 'labcro1': 100, 'laudov1': 101, 'lblwar1': 102, 'lesyel1': 103, 'lewduc1': 104, 'lirplo': 105, 'litegr': 106, 'litgre1': 107, 'litspi1': 108, 'litswi1': 109, 'lobsun2': 110, 'maghor2': 111, 'malpar1': 112, 'maltro1': 113, 'malwoo1': 114, 'marsan': 115, 'mawthr1': 116, 'moipig1': 117, 'nilfly2': 118, 'niwpig1': 119, 'nutman': 120, 'orihob2': 121, 'oripip1': 122, 'pabflo1': 123, 'paisto1': 124, 'piebus1': 125, 'piekin1': 126, 'placuc3': 127, 'plaflo1': 128, 'plapri1': 129, 'plhpar1': 130, 'pomgrp2': 131, 'purher1': 132, 'pursun3': 133, 'pursun4': 134, 'purswa3': 135, 'putbab1': 136, 'redspu1': 137, 'rerswa1': 138, 'revbul': 139, 'rewbul': 140, 'rewlap1': 141, 'rocpig': 142, 'rorpar': 143, 'rossta2': 144, 'rufbab3': 145, 'ruftre2': 146, 'rufwoo2': 147, 'rutfly6': 148, 'sbeowl1': 149, 'scamin3': 150, 'shikra1': 151, 'smamin1': 152, 'sohmyn1': 153, 'spepic1': 154, 'spodov': 155, 'spoowl1': 156, 'sqtbul1': 157, 'stbkin1': 158, 'sttwoo1': 159, 'thbwar1': 160, 'tibfly3': 161, 'tilwar1': 162, 'vefnut1': 163, 'vehpar1': 164, 'wbbfly1': 165, 'wemhar1': 166, 'whbbul2': 167, 'whbsho3': 168, 'whbtre1': 169, 'whbwag1': 170, 'whbwat1': 171, 'whbwoo2': 172, 'whcbar1': 173, 'whiter2': 174, 'whrmun': 175, 'whtkin2': 176, 'woosan': 177, 'wynlau1': 178, 'yebbab1': 179, 'yebbul3': 180, 'zitcis1': 181}\n",
    "}"
   ],
   "id": "7ecc955808518df2",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Utils",
   "id": "c1254d14c0c06ace"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T15:07:25.023140Z",
     "start_time": "2024-04-28T15:07:25.019556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def setup_output_dir(config):\n",
    "    os.makedirs(config[\"output_folder\"], exist_ok=True)\n",
    "    exp_folder = os.path.join(config[\"output_folder\"], config[\"exp_name\"])\n",
    "    os.makedirs(exp_folder, exist_ok=True)\n",
    "    config[\"exp_folder\"] = exp_folder\n",
    "    return config\n",
    "\n",
    "def normalize_rating(df):\n",
    "    return np.clip(df[\"rating\"] / df[\"rating\"].max(), 0.1, 1.0)\n",
    "\n",
    "\n",
    "def do_kfold(df, KFOLD=5):\n",
    "    skf = StratifiedKFold(n_splits=KFOLD, random_state=config[\"seed\"], shuffle=True)\n",
    "    df['fold'] = -1\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X=df, y=df[\"primary_label\"].values)):\n",
    "        df.loc[val_idx, 'fold'] = fold\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_dataframe():\n",
    "    df = pd.read_csv('../data/birdclef-2024/train_metadata.csv')\n",
    "    return df\n"
   ],
   "id": "7d1c049c4f364161",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T15:07:25.217341Z",
     "start_time": "2024-04-28T15:07:25.215161Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "bd4b11e6bd953a3a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataframe Prep",
   "id": "84702f6a10f60ffd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T15:07:25.663539Z",
     "start_time": "2024-04-28T15:07:25.612042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config = setup_output_dir(config)\n",
    "df = read_dataframe()\n",
    "set_seed(config[\"seed\"])\n",
    "df[\"rating\"] = normalize_rating(df)\n",
    "df = do_kfold(df, KFOLD=5)"
   ],
   "id": "33332066a4b9de68",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train Fold",
   "id": "534b977c77f4eb2e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T15:07:26.023997Z",
     "start_time": "2024-04-28T15:07:26.012379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fold = config[\"fold\"]\n",
    "logger = init_logger(log_file=os.path.join(config[\"exp_folder\"], f\"{fold}.log\"))\n",
    "\n",
    "\n",
    "logger.info(\"=\" * 90)\n",
    "logger.info(f\"Fold {fold} Training\")\n",
    "logger.info(\"=\" * 90)\n",
    "\n",
    "\n",
    "trn_df = df[df['fold'] != fold].reset_index(drop=True)\n",
    "val_df = df[df['fold'] == fold].reset_index(drop=True)\n",
    "print(trn_df.shape)\n",
    "logger.info(trn_df.shape)\n",
    "logger.info(trn_df['primary_label'].value_counts())\n",
    "logger.info(val_df.shape)\n",
    "logger.info(val_df['primary_label'].value_counts())\n",
    "\n"
   ],
   "id": "c18f645ecac79699",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Fold 3 Training\n",
      "==========================================================================================\n",
      "(19567, 13)\n",
      "primary_label\n",
      "zitcis1    400\n",
      "lirplo     400\n",
      "litgre1    400\n",
      "comgre     400\n",
      "comkin1    400\n",
      "          ... \n",
      "paisto1      5\n",
      "niwpig1      4\n",
      "asiope1      4\n",
      "integr       4\n",
      "blaeag1      4\n",
      "Name: count, Length: 182, dtype: int64\n",
      "(4892, 13)\n",
      "primary_label\n",
      "zitcis1    100\n",
      "lirplo     100\n",
      "comgre     100\n",
      "comkin1    100\n",
      "commoo3    100\n",
      "          ... \n",
      "integr       1\n",
      "asiope1      1\n",
      "wynlau1      1\n",
      "nilfly2      1\n",
      "niwpig1      1\n",
      "Name: count, Length: 182, dtype: int64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19567, 13)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prepare Data Loaders",
   "id": "53eae506f075bbf2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T15:07:26.472811Z",
     "start_time": "2024-04-28T15:07:26.447601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "labels = trn_df[\"primary_label\"].values\n",
    "un_labels = np.unique(labels)\n",
    "weight = {t: 1.0 / len(np.where(labels == t)[0]) for t in un_labels}\n",
    "samples_weight = np.array([weight[t] for t in labels])\n",
    "sampler = WeightedRandomSampler(torch.from_numpy(samples_weight).type('torch.DoubleTensor'),\n",
    "                                len(samples_weight))"
   ],
   "id": "26497e1afb0ef73a",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T15:07:26.778208Z",
     "start_time": "2024-04-28T15:07:26.699398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trn_dataset = BirdDataset(df=trn_df.reset_index(drop=True), config=config, num_classes=len(config[\"target_columns\"]), add_secondary_labels=True)\n",
    "train_loader = torch.utils.data.DataLoader(trn_dataset, shuffle=False, sampler=sampler,\n",
    "                                                   **config[\"train_loader_config\"])\n",
    "\n",
    "v_ds = BirdDataset(df=val_df.reset_index(drop=True), config=config, num_classes=len(config[\"target_columns\"]), add_secondary_labels=False)\n",
    "val_loader = torch.utils.data.DataLoader(v_ds, shuffle=False, **config[\"val_loader_config\"])"
   ],
   "id": "468aa768996a68fb",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T15:07:26.976774Z",
     "start_time": "2024-04-28T15:07:26.974644Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "3123761067f21f48",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loss Function",
   "id": "32208684a09c4a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T15:07:27.789774Z",
     "start_time": "2024-04-28T15:07:27.786287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha: float = 0.25,\n",
    "        gamma: float = 2,\n",
    "        reduction: str = \"mean\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, x):\n",
    "        inputs = x[\"logit\"]\n",
    "        targets = x[\"primary_targets\"]        \n",
    "        return torchvision.ops.focal_loss.sigmoid_focal_loss(\n",
    "            inputs=inputs,\n",
    "            targets=targets,\n",
    "            alpha=self.alpha,\n",
    "            gamma=self.gamma,\n",
    "            reduction=self.reduction,\n",
    "        )"
   ],
   "id": "5d6baf85260e7437",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model",
   "id": "665a205289a39abc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T15:07:30.654460Z",
     "start_time": "2024-04-28T15:07:28.274704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = CNN(config).to(config[\"device\"])\n",
    "criterion = FocalLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr_max\"], betas=(0.9, 0.999), eps=1e-08, weight_decay=config[\"weight_decay\"], amsgrad=False, )\n",
    "scheduler = CosineLRScheduler(optimizer, t_initial=10, warmup_t=1, cycle_limit=40, cycle_decay=1.0, lr_min=config[\"lr_min\"], t_in_epochs=True, )"
   ],
   "id": "4d2886a7ee3b7c15",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature dims: [1536, 2304]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train / Val Functions",
   "id": "22481d3a8bced627"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T15:07:30.657162Z",
     "start_time": "2024-04-28T15:07:30.654460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def batch_to_device(batch, device):\n",
    "    batch_dict = {key: batch[key].to(device) for key in batch}\n",
    "    return batch_dict"
   ],
   "id": "9e83d10be6ec7b94",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T15:07:30.664957Z",
     "start_time": "2024-04-28T15:07:30.657162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_one_epoch(data_loader, model, criterion, optimizer, scheduler, epoch, device, apex,\n",
    "                             max_grad_norm, target_columns):\n",
    "\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    scaler = GradScaler(enabled=apex)\n",
    "    iters = len(data_loader)\n",
    "    gt = []\n",
    "    preds = []\n",
    "    with tqdm(enumerate(data_loader), total=len(data_loader)) as t:\n",
    "        for i, (batch) in t:\n",
    "            batch = batch_to_device(batch, device)\n",
    "\n",
    "            with autocast(enabled=apex):\n",
    "                outputs = model(batch)\n",
    "                loss = criterion(outputs)\n",
    "\n",
    "            losses.update(loss.item(), batch[\"wave\"].size(0))\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(), max_norm=max_grad_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step(epoch + i / iters)\n",
    "            t.set_postfix(\n",
    "                loss=losses.avg,\n",
    "                grad=grad_norm.item(),\n",
    "                lr=optimizer.param_groups[0][\"lr\"]\n",
    "            )\n",
    "            gt.append(batch[\"primary_targets\"].cpu().detach().numpy())\n",
    "            preds.append(outputs[\"logit\"].sigmoid().cpu().detach().numpy())\n",
    "\n",
    "    gt = np.concatenate(gt)\n",
    "    preds = np.concatenate(preds)\n",
    "    scores = calculate_competition_metrics(gt, preds, target_columns)\n",
    "    return scores, losses.avg"
   ],
   "id": "df461861d92b3f84",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T15:07:30.671206Z",
     "start_time": "2024-04-28T15:07:30.664957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def validate_one_epoch(data_loader, model, criterion, device, apex, target_columns):\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    gt = []\n",
    "    preds = []\n",
    "\n",
    "    with tqdm(enumerate(data_loader), total=len(data_loader)) as t:\n",
    "        for i, (batch) in t:\n",
    "            batch = batch_to_device(batch, device)\n",
    "            with autocast(enabled=apex):\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(batch)\n",
    "                    loss = criterion(outputs)\n",
    "\n",
    "            losses.update(loss.item(), batch[\"wave\"].size(0))\n",
    "            t.set_postfix(loss=losses.avg)\n",
    "\n",
    "            gt.append(batch[\"primary_targets\"].cpu().detach().numpy())\n",
    "            preds.append(batch[\"logit\"].sigmoid().cpu().detach().numpy())\n",
    "\n",
    "    gt = np.concatenate(gt)\n",
    "    preds = np.concatenate(preds)\n",
    "    scores = calculate_competition_metrics(gt, preds, target_columns)\n",
    "    return scores, losses.avg\n"
   ],
   "id": "b2934075e8450208",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train Fold",
   "id": "b7d87564112721e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T15:07:30.679334Z",
     "start_time": "2024-04-28T15:07:30.671206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    patience = config[\"early_stopping\"]\n",
    "    best_score = 0.0\n",
    "    n_patience = 0\n",
    "    \n",
    "    for epoch in range(1, config[\"epochs\"] + 1):\n",
    "    \n",
    "        train_scores, train_losses_avg = train_one_epoch(data_loader=train_loader, model=model,\n",
    "                                                  criterion=criterion, optimizer=optimizer,\n",
    "                                                  scheduler=scheduler,\n",
    "                                                  epoch=0, device=config[\"device\"],\n",
    "                                                  apex=config[\"apex\"],\n",
    "                                                  max_grad_norm=config[\"max_grad_norm\"],\n",
    "                                                  target_columns=config[\"target_columns\"])\n",
    "    \n",
    "        train_scores_str = metrics_to_string(train_scores, \"Train\")\n",
    "        train_info = f\"Epoch {epoch} - Train loss: {train_losses_avg:.4f}, {train_scores_str}\"\n",
    "        logger.info(train_info)\n",
    "    \n",
    "        val_scores, val_losses_avg = validate_one_epoch(data_loader=val_loader, model=model, criterion=criterion, device=config[\"device\"],\n",
    "                                            apex=config[\"apex\"], target_columns=config[\"target_columns\"])\n",
    "    \n",
    "        val_scores_str = metrics_to_string(val_scores, f\"Valid\")\n",
    "        val_info = f\"Epoch {epoch} - Valid loss: {val_losses_avg:.4f}, {val_scores_str}\"\n",
    "        logger.info(val_info)\n",
    "    \n",
    "        val_score = val_scores[\"ROC\"]\n",
    "    \n",
    "        is_better = val_score > best_score\n",
    "        best_score = max(val_score, best_score)\n",
    "    \n",
    "        exp_name = config[\"exp_name\"]\n",
    "    \n",
    "        if is_better:\n",
    "            state = {\n",
    "                \"epoch\": epoch,\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"best_loss\": best_score,\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "            }\n",
    "            logger.info(\n",
    "                f\"Epoch {epoch} - Save Best Score: {best_score:.4f} Model\\n\")\n",
    "            torch.save(\n",
    "                state,\n",
    "                os.path.join(config[\"exp_folder\"], f\"{fold}.bin\")\n",
    "            )\n",
    "            n_patience = 0\n",
    "        else:\n",
    "            n_patience += 1\n",
    "            logger.info(\n",
    "                f\"Valid loss didn't improve last {n_patience} epochs.\\n\")\n",
    "    \n",
    "        if n_patience >= patience:\n",
    "            logger.info(\n",
    "                \"Early stop, Training End.\\n\")\n",
    "            state = {\n",
    "                \"epoch\": epoch,\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"best_loss\": best_score,\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "            }\n",
    "            torch.save(\n",
    "                state,\n",
    "                os.path.join(config[\"exp_folder\"], f\"final_{fold}.bin\")\n",
    "            )\n",
    "            break"
   ],
   "id": "dd5559e55b19425f",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-04-28T15:07:30.679334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "dc99619e27c61646",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3f54fe3bb402b27a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
